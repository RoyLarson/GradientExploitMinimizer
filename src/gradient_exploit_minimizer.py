"""
This module is the code for the gradient_exploiting_minimizer
"""
from __future__ import annotations
from functools import lru_cache
from typing import Callable, List, Tuple, Optional

import numpy as np


class BoundaryError(Exception):
    pass


class Minimizer:
    def __init__(
        self,
        fun: Callable,
        grad: Callable,
        bounds: Optional[List[Tuple[float, float]]] = None,
        f_tol: float = 1e-9,
        g_tol: float = 1e-6,
        exploit_tolerance=1e-4,
        learning_rate: float = 1,
        max_iters=1000,
    ) -> None:
        self.fun = fun
        self.grad = grad
        self.bounds = bounds
        self.f_tol = f_tol
        self.g_tol = g_tol
        self.exp_tol = exploit_tolerance
        self.learning_rate = learning_rate
        self.max_iters = max_iters
        self.current_min = None
        self.best_min = None
        self.current_x = None
        self.best_x = None

    def convergence_check(self, new_min: float, new_x: np.ndarray, grad_x:np.ndarray) -> bool:
        """Check to see if minimization has been found"""
        f_tol_converged = (self.current_min - new_min) / max(
            [abs(self.current_min), abs(new_min), 1]
        )<self.f_tol
        g_tol_converged = (
            max(map(abs, (xi - x0 for xi, x0 in zip(new_x, self.best_x)))) < self.g_tol
        )
        grad_converged = max(np.abs(grad_x))<self.g_tol
        return f_tol_converged or g_tol_converged or grad_converged 

    def find_rhs(
        self, new_x: np.ndarray
    ) -> Tuple[np.ndarray, List[Tuple[float, float]]]:
        eps = 1e-4
        vector = new_x - self.current_x
        left_lambda = 0
        right_lambda = 1
        lambda_ = 0.5
        points = [(0, self.current_min)]
        curr_min = self.current_min
        while True:
            test_x = lambda_ * vector + self.current_x
            test_result = self.fun(*test_x)
            # TODO if right_lambda-left_lambda <eps rescale and restart
            if test_result == np.inf:  # Haven't moved far enough left
                if right_lambda-left_lambda<eps: #rescale the vector
                    vector = right_lambda*vector
                    points = [(lamb/right_lambda, result) for lamb, result in points]
                    right_lambda = 1
                    lambda_ = .5
                else:
                    right_lambda = lambda_
            elif (
                abs(lambda_ - right_lambda) < eps
            ):  # Down hill all the way, with sharp edge
                points.append((lambda_, test_result))
                break
            elif (
                test_result < curr_min
            ):  # There might be a better point to the right so can't stop yet
                left_lambda = lambda_
                curr_min = test_result
                points.append((lambda_, test_result))
            elif (
                test_result > curr_min
            ):  # found an uphill section so we can stop because the gradient wouldn't point us in this direction any more
                # because the gradient told the algorithm to go in this direction
                points.append((lambda_, test_result))
                break

            lambda_ = (right_lambda + left_lambda) / 2  # normal bisection search
        points.sort(key=lambda x: x[0])
        right_lambda, _ = points[-1]
        new_x = right_lambda * vector + self.current_x
        scaled_points = [(lamb / right_lambda, result) for lamb, result in points]
        return new_x, scaled_points

    def calculate_intersetion(
        self,
        left_point: Tuple[float, float],
        right_point: Tuple[float, float],
        slope: float,
    ) -> Tuple[float, float]:
        """Uses the left_point, right_point and slope to calculate an new x, y

        x1,y1                x2,y2
         \-       x,y       -/   
          \ -    - -       -/
           \  -    | -    -/
            \      |  -  -/
             \     |   - /
              \    |    /
               \   |   /
                \  |  /
                 \ | /
                   X
    

        Args:
            left_point: x,y on the left to start from
            right_point: x, y on the right to start from
            slope: absolute value of the slope to use to draw lines down from
        
        Returns:
            x, y of the intersetion
        
        """
        x1, y1 = left_point
        x2, y2 = right_point
        x = ((y1 - y2) + slope * x1 + slope * x2) / (2 * slope)
        y = y1 - slope * x + slope * x1
        return x, y

    def exploit_gradient(self, new_x: np.ndarray) -> Tuple[float, np.ndarray]:
        """Use Schubert's algorithm to search between new_x and current_x to find 
        approximate min along line.  
        
        This way when the gradient is called next it will go almost perpendicular to this direction.

        Hopefully this saves time.
        Schubert's algorithm
        Take two points with known x, y where a minimum happens between them
        given a maximum slope between the two linearly project the left point 
        with -slope 
        linearly project the right point with positive slope, test the point at the intersetion
        check for new gradient, if the gradient is too small, may not find the best point
        if the gradient is too big will use a lot of iterations.
    
        x1,y1                x2,y2              x1,y1                x3,y3
         \-       x,y       -/                    \-       x2,y2      -/
          \ -    - -       -/                      \ -    - -        -/
           \  -    | -    -/                        \  -   / \ -    -/
            \      |  -  -/            ->            \ |  /   \ -  -/
             \     |   - /                            \| /     \ - /
              \    |    /                              X        \|/
               \   |   /                                         X
                \  |  /
                 \ | /
                   X
    
        Should work as long as 
    

        So adapt the search using the steepest slope found.

        Args:
            New point projected by the gradient to search at

        Returns:
            min_x, f_min
            approximate minimum result in this direction
            the location of the minimum result
        """
        new_min = self.fun(*new_x)
        if new_min == np.inf:
            new_x, point_results = self.find_rhs(new_x)
        else:
            point_results = [(0, self.current_min), (1, new_min)]
        vector = new_x - self.current_x

        max_slope = max(
            (
                2 * (abs((y2 - y1) / (x2 - x1)))
                for (y2, x2), (y1, x1) in zip(point_results, point_results[1:])
            )
        )  # multiply by 2 to make sure that we get some point inbetween
        tried = set()
        proj_points = {}
        while True:
            new_points = {(left_point, right_point, max_slope):
                self.calculate_intersetion(left_point, right_point, max_slope)
                for left_point, right_point in zip(point_results, point_results[1:])
                if right_point[0] - left_point[0] > self.exp_tol and (left_point, right_point, max_slope) not in proj_points
            }
            proj_points = {**proj_points, **new_points}
            for k, (lamb, y) in sorted(proj_points.items(), key=lambda m: m[-1][1]):  # Sort by y
                if k not in tried:
                    min_lamb = lamb
                    tried.add(k)
            new_point = (min_lamb, self.fun(min_lamb*vector+self.current_x))
            point_results.append(new_point)
            point_results.sort(
                key=lambda x: x[0]
            )  # I know this is the default by I like being explicit
            max_slope = max(
                (
                    2 * (abs((y1 - y2) / (x1 - x2)))
                    for (x1, y1), (x2, y2) in zip(point_results, point_results[1:])
                )
            )  # multiply by 2 to make sure we can get some point between the current best
            if len(point_results) > 2 * len(
                new_x
            ):  # Have already evaluated as many times as a central difference gradient
                break
        lambda_, fun_min = min(point_results, key=lambda x: x[1])
        min_x = lambda_ * vector + self.current_x
        return min_x, fun_min

    def apply_bounds(self, x: np.ndarray) -> np.ndarray:
        """Use a linear projection to find closest bound, scale current vector to stop at closest bound.
        Args:
            x: the projected new location

        Returns:
            scaled x that satisfies the boundaries
        """
        eps = 1e-4  # any x that is closer than this to the boundary needs to be rotated
        x = [
            xi if abs(ub - x0) > eps else ub
            for (_, ub), xi, x0 in zip(self.bounds, x, self.current_x)
        ]  # Makes the direction of x parrallel to upper boundary if already close
        x = [
            xi if abs(lb - x0) > eps else lb
            for (lb, _), xi, x0 in zip(self.bounds, x, self.current_x)
        ]  # Makes the direction of x parrallel to lower boundary if already close
        vector = x - self.current_x
        if (
            sum((vi ** 2 for vi in vector)) < eps
        ):  # gradient is pointing directly into boundary so cannot move any better
            raise BoundaryError(f"Stuck on boundary: {x}")

        ub_lambdas = [
            abs((ub - x0) / (xi - x0))
            for (_, ub), xi, x0 in zip(self.bounds, x, self.current_x)
            if xi > ub
        ]
        lb_lambdas = [
            abs((lb - x0) / (xi - x0))
            for (lb, _), xi, x0 in zip(self.bounds, x, self.current_x)
            if xi < lb
        ]
        lambda_ = min(ub_lambdas + lb_lambdas)
        return np.array(lambda_ * vector + self.current_x)

    def solve(self, x0: np.ndarray) -> Tuple[float, np.ndarray]:
        """Minimize the function given the starting point x0

        Arg:
            x0: the starting point for the run

        Returns:
            minimum value
            the location of the minimum value found

        """
        if (self.bounds is not None) and (len(self.bounds) != len(x0)):
            raise Exception(
                f"lenght of bounds!=length of x0: {len(self.bounds)} != {len(x0)}"
            )
        if not all((lb<=x<=ub for (lb, ub), x in zip(self.bounds, x0))):
            raise Exception("x0 not in bounds")
        x = np.array(list(x0))
        min_result = self.fun(*x)
        if not (-np.inf < min_result < np.inf):
            raise Exception(f"Evaluation failed on initial x0: {x0}")
        self.current_x = x
        self.current_min = min_result
        self.best_x = x
        self.best_min = min_result
        i = 0
        while True:
            grad_x = self.grad(*self.current_x)
            new_x = x - self.learning_rate * grad_x  # initial direction in gradient decent
            if (self.bounds is not None) and not all(
                (lb < xi < ub for (lb, ub), xi in zip(self.bounds, new_x))
            ):
                try:
                    new_x = self.apply_bounds(new_x)
                except BoundaryError as err:
                    print(err)
                    # Don't worry about saving the new_x because it is effectively self.current_x
                    break
            new_min, new_x = self.exploit_gradient(new_x)
            if new_min < self.best_min:
                self.best_min = new_min
                self.best_x = new_x
            if self.convergence_check(new_min, new_x):
                break
            else:
                self.current_x = new_x
                self.current_min = new_min
            i += 1
            if i > self.max_iters:
                break

        return self.best_min, self.best_x
