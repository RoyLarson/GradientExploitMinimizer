"""
This module is the code for the gradient_exploiting_minimizer
"""
from functools import lru_cache
from typing import Callable, List, Tuple, Optional

import numpy as np

class BoundaryError(Exception):
    pass

class Minimzer:
    def __init__(self, fun:Callable, grad:Callable, bounds:Optional[List[Tuple[float, float]]]=None, f_tol:float=1e-9, g_tol:float=1e-6, exploit_tolerance=1e-4,learning_rate:float=1, max_iters = 1000)->None:
        self.fun = fun
        self.grad = grad
        self.bounds =bounds
        self.f_tol = f_tol
        self.g_tol = g_tol
        self.exp_tol = exploit_tolerance
        self.learning_rate = learning_rate
        self.max_iters = max_iters
        self.current_min = None
        self.best_min = None
        self.current_x = None
        self.best_x = None

    def convergence_check(self, new_min:float, new_x:np.ndarray)->bool:
        """Check to see if minimization has been found"""
        f_tol_converged = (self.current_min-new_min)/max([abs(self.current_min), abs(new_min), 1])
        g_tol_converged = max(map(abs, (xi-x0 for xi, x0 in zip(new_x, self.best_x))))<self.g_tol
        return f_col_converged or g_tol_converged

    def find_rhs(self, new_x):
        vector = new_x-self.current_x
        left_lambda = 0
        right_lambda = 1
        lambda_ = .5
        while True:
            test_x = lambda_*vector+self.current_x
            test_result = self.fun(test_x)
            if test_result==np.inf:   # Haven't moved far enough left
                right_lambda=lambda_
            elif test_result<self.current_min:  # There might be a better point to the right so can't stop yet
                left_lambda = lambda_
            else: # test_result is at least as bad as self.current_min so the answer must be between these two points
                  # because the gradient told the algorithm to go in this direction
                return test_x
            lambda_ = (right_lambda+left_lambda)/2 # normal bisection search
    
    def calculate_intersetion(self, left_point:Tuple[float, float], right_point:Tuple[float, float], slope:float)->Tuple[float, float]:
        """Uses the left_point, right_point and slope to calculate an new x, y

        x1,y1                x2,y2
         \-       x,y       -/
          \ -    - -       -/
           \  -    |-     -/
            \      | -   -/
             \     |   - /
              \    |    /
               \   |   /
                \  |  /
                 \ | /
                   X
    
        Args:
            left_point: x,y on the left to start from
            right_point: x, y on the right to start from
            slope: absolute value of the slope to use to draw lines down from
        
        Returns:
            x, y of the intersetion
        
        """
        x1, y1 = left_point
        x2, y2 = right_point
        x =((y1-y2)+slope*x1+slope*x2)/(2*slope)
        y = y1-slope*x+slope*x1
        return x, y

            
    def exploit_gradient(self, new_x:np.ndarray)->Tuple[float, np.ndarray]:
        """Use Schubert's algorithm to search between new_x and current_x to find 
        approximate min along line.  
        
        This way when the gradient is called next it will go almost perpendicular to this direction.

        Hopefully this saves time.
        Schubert's algorithm
        Take two points with known x, y where a minimum happens between them
        given a maximum slope between the two linearly project the left point 
        with -slope 
        linearly project the right point with positive slope, test the point at the intersetion
        check for new gradient, if the gradient is too small, may not find the best point
        if the gradient is too big will use a lot of iterations.

        So adapt the search using the steepest slope found.

        Args:
            New point projected by the gradient to search at

        Returns:
            approximate minimum result in this direction
            the location of the minimum result
        """
        new_min = self.fun(*new_x)
        if new_min == np.inf:
            new_x, new_min = self.find_rhs(new_x)
        vector = new_x-self.current_x
        point_results = [(0, self.current_min), (1, new_min)]
        max_slope = 1
        current_min = min([self.current_min, new_min])
        while True:
            proj_points = [self.calculate_intersetion(left_point, right_point) for left_point, right_point in zip(point_results, point_results[1:]) if right_point[0]-left_point[0]>self.exp_tol]
            proj_points = [point for point in proj_points if point[1]<current_min]
            if not proj_points: # no place else to search between
                break
            new_points = [(lam_, self.fun(*(lam*vector+self.current_x))) for lam, _ in proj_points]
            point_results += new_points
            points_results.sort(key= lambda x: x[0]) # I know this is the default by I like being explicit
            max_slope = max((abs((y1-y2)/(x1-x2)) for (x1, y1), (x2, y2) in zip(point_results, point_results[1:])))
            if len(point_results)>2*len(x):  # Have already evaluated as many times as a central difference gradient
                break
        lambda_, fun_min = min(point_results, key = lambda x: x[1])
        min_x = min_point[0]*vector+self.current_x
        return min_x, fun_min

    def apply_bounds(self, x:np.ndarray)->np.ndarray:
        """Use a linear projection to find closest bound, scale current vector to stop at closest bound.
        Args:
            x: the projected new location

        Returns:
            scaled x that satisfies the boundaries
        """
        eps = 1e-4 # any x that is closer than this to the boundary needs to be rotated
        x = [xi if abs(ub-x0)>eps else ub for (_, ub), xi, x0 in zip(bounds, x, self.current_x)] # Makes the direction of x parrallel to upper boundary if already close
        x = [xi if abs(lb-x0)>eps else lb for (lb, _), xi, x0 in zip(bounds, x, self.current_x)] # Makes the direction of x parrallel to lower boundary if already close
        vector = x-self.current_x
        if sum((vi**2 for vi in vector))<eps: #gradient is pointing directly into boundary so cannot move any better
            raise BoundaryError(f"Stuck on boundary: {x}")

        ub_lambdas = [abs((ub-x0)/(xi-x0))  for (_, ub), xi, x0 in zip(bounds, x, self.current_x) if xi>ub]
        lb_lambdas = [abs((lb-x0)/(xi-x0)) for (lb, _), xi, x0 in zip(bounds, x, self.current_x) if xi<lb]
        lambda_ = min(ub_lambdas+lb_lambdas)
        return np.array(lambda_*(vector)+self.current_x)

    def solve(self, x0:np.ndarray)->Tuple[float, np.ndarry]:
        """Minimize the function given the starting point x0

        Arg:
            x0: the starting point for the run

        Returns:
            minimum value
            the location of the minimum value found

        """
        if (self.bounds is not None) and (len(self.bounds)!=len(x0)):
            raise Exception(f"lenght of bounds!=length of x0: {len(self.bounds)} != {len(x0)}")
        x = np.array(list(x0))
        min_result = self.fun(*x)
        if not (np.inf<min_result<np.inf):
            raise Exception(f"Evaluation failed on initial x0: {x0}")
        self.current_x = x
        self.current_min = min_result
        i = 0
        while True:
            new_x = x-self.learning_rate*self.grad(*x) #initial direction in gradient decent
            if (self.boundas is not None) and not (all((lb<xi<ub for (lb, ub), xi in zip(self.bounds, new_x))):
                try:
                    new_x = self.apply_bounds(new_x)
                except BoundaryError as err:
                    print(err)
                    # Don't worry about saving the new_x because it is effectively self.current_x
                    break
            new_min, new_x = self.exploit_gradient(new_x)
            if new_min < self.best_min:
                self.best_min = new_min
                self.best_x = new_x
            if self.convergence_check(new_min, new_x):
                break
            else:
                self.current_x = new_x
                self.current_min = new_min
            i += 1
            if i>self.max_iters:
                break

        return self.best_min, self.best_x

