"""
This module is the code for the gradient_exploiting_minimizer
"""
from typing import Callable, List, Tuple

import numpy as np

class Minimzer:
    def __init__(self, fun:Callable, grad:Callable, bounds:List[Tuple[float, float]]=None, f_tol:float=1e-9, g_tol:float=1e-6, learning_rate:float=1, max_iters = 1000)->None:
        self.fun = fun
        self.grad = grad
        self.bounds =bounds
        self.f_tol = f_tol
        self.g_tol = g_tol
        self.learning_rate = learning_rate
        self.max_iters = max_iters
        self.current_min = None
        self.best_min = None
        self.current_x = None
        self.best_x = None

    def convergence_check(self, new_min:float, new_x:np.ndarray)->bool:
        """Check to see if minimization has been found"""
        f_tol_converged = (self.current_min-new_min)/max([abs(self.current_min), abs(new_min), 1])
        g_tol_converged = max(map(abs, (xi-x0 for xi, x0 in zip(new_x, self.best_x))))<self.g_tol
        return f_col_converged or g_tol_converged

    def find_rhs(self, new_x):
        vector = new_x-self.current_x
        left_lambda = 0
        right_lambda = 1
        lambda_ = .5
        while True:
            test_x = lambda_*vector+self.current_x
            test_result = self.fun(test_x)
            if test_result==np.inf:   # Haven't moved far enough left
                right_lambda=lambda_
            elif test_result<self.current_min:  # There might be a better point to the right so can't stop yet
                left_lambda = lambda_
            else: # test_result is at least as bad as self.current_min so the answer must be between these two points
                  # because the gradient told the algorithm to go in this direction
                return test_x
            lambda_ = (right_lambda+left_lambda)/2 # normal bisection search
            
    def exploit_gradient(self, new_x:np.ndarray)->Tuple[float, np.ndarray]:
        """Use Schubert's algorithm to search between new_x and current_x to find 
        approximate min along line.  
        
        This way when the gradient is called next it will go almost perpendicular to this direction.

        Hopefully this saves time.
        Schubert's algorithm
        Take two points with known x, y where a minimum happens between them
        given a maximum slope between the two linearly project the left point 
        with -slope 
        linearly project the right point with positive slope, test the point at the intersetion
        check for new gradient, if the gradient is too small, may not find the best point
        if the gradient is too big will use a lot of iterations.

        So adapt the gradient using the steepest gradient found.

        Args:
            New point projected by the gradient to search at

        Returns:
            approximate minimum result in this direction
            the location of the minimum result
        """
        new_min = self.fun(*new_x)
        if new_min == np.inf:
            new_x, new_min = self.find_rhs(new_x)
        vector = new_x-self.current_x
        point_results = [(0, self.current_min), (1, new_min)]
        max_slope = 1
        current_min = min([self.current_min, new_min])
        while True:
            new_points = []
            updated_min = False
            for left_point, right_point in zip(point_results, point_results[1:]):
                new_lambda = ((left_point[1]-right_point[1])+max_slope*left_point[0]+max_slope*right_point[0])/(2*m)
                new_x = new_lambda*vector+self.current_x
                new_min = self.fun(*new_x)
                new_points.append((new_lambda, new_min))
                left_slope = abs((left_point[1]-new_min)/(left_point[0]-new_lambda))
                right_slope = abs((right_point[1]-new_min)/(right_point[0]-new_lambda))
                max_slope = max([max_slope, left_slope, right_slope])
                if new_min<current_min:
                    updated_min = True
                    current_min = new_min
            if not updated_min:
                break
            point_results += new_points
            point_results.sort(key = lambda x:x[0]) # I know this is the default by I like being explicit
            if len(point_results)>2*len(x):  # Have already evaluated as many times as a central difference gradient
                break
        min_point = min(point_results, key = lambda x: x[1])
        min_x = min_point[0]*vector+self.current_x
        return min_point[1], min_x

    def apply_bounds(self, x):
        vector = x-self.current_x
        ub_lambdas = [(ub-x0)/(xi-x0) for (_, ub), xi, x0 in zip(bounds, x, self.current_x) if xi>ub]
        lb_lambdas = [abs((lb-x0)/(xi-x0)) for (lb, _), xi, x0 in zip(bounds, x, self.current_x) if xi<lb]
        lambda_ = min(ub_lambdas+lb_lambdas)
        return lambda_*(vector)+self.current_x



    def solve(self, x0):
        x = np.array(list(x0))
        min_result = self.fun(*x)
        if not (np.inf<min_result<np.inf):
            raise Exception(f"Evaluation failed on initial x0: {x0}")
        self.current_x = x
        self.current_min = min_result
        i = 0
        while True:
            new_x = x-self.learning_rate*self.grad(*x)
            if not all((lb<xi<ub for (lb, ub), xi in zip(self.bounds, new_x)):
                new_x = self.apply_bounds(new_x)
            new_min, new_x = self.exploit_gradient(new_x)
            if self.convergence_check(new_min, new_x):
                break
            else:
                self.current_x = new_x
                self.current_min = new_min
            if self.current_min<self.best_min:
                self.best_min = self.current_min
                self.best_x = self.current_x
            i += 1
            if i>self.max_iters:
                break

        return self.best_min, self.best_x

