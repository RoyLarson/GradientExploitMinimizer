# Gradient Exploiting Minimizer
This is developed because I have a multivariate nonlinear minimization problem  
where there is not an analytical solution to the gradient.  
A finite difference can be done on the problem to get the gradient, but each  
evaluation is a few seconds, so getting as much out of each gradient step as possible  
is necessary.  It is also possible in my situation that the problem jumps outside of
the allowable space that cannot be set by just bounds.  In this case the code returns 
np.inf

## The idea
Take an initial point, bounds, function, gradient_funtion, a learning rate
then use Schubert's Algorithm to find a point close to the minimum along the gradient
Then repeat


